{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'talk.religion.misc','comp.graphics', 'sci.space']\n",
    "categories = None\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "from pprint import pprint\n",
    "pprint(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting text to vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (11314, 5000)\n",
      "x_test shape: (7532, 5000)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(newsgroups_train[\"data\"])\n",
    "x_train = tokenizer.texts_to_matrix(newsgroups_train[\"data\"], mode='binary')\n",
    "x_test = tokenizer.texts_to_matrix(newsgroups_test[\"data\"], mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 classes\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.max(newsgroups_train[\"target\"]) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert class vector to binary class matrix (for use with categorical_crossentropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (11314, 20)\n",
      "y_test shape: (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(newsgroups_train[\"target\"], num_classes)\n",
    "y_test = keras.utils.to_categorical(newsgroups_test[\"target\"], num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building model functionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Input(shape=(max_words,))\n",
    "b = Dense(512)(a)\n",
    "b = Activation('relu')(b)\n",
    "b = Dropout(0.5)(b)\n",
    "b = Dense(num_classes)(b)\n",
    "b = Activation('softmax')(b)\n",
    "model = Model(inputs=a, outputs=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x26a809c7da0>,\n",
       " <keras.layers.core.Dense at 0x26a80747278>,\n",
       " <keras.layers.core.Activation at 0x26a8263eda0>,\n",
       " <keras.layers.core.Dropout at 0x26a82647fd0>,\n",
       " <keras.layers.core.Dense at 0x26a82647f98>,\n",
       " <keras.layers.core.Activation at 0x26a82647b70>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: tensorflow\n",
      "class_name: Model\n",
      "config:\n",
      "  input_layers:\n",
      "  - [input_9, 0, 0]\n",
      "  layers:\n",
      "  - class_name: InputLayer\n",
      "    config:\n",
      "      batch_input_shape: !!python/tuple [null, 5000]\n",
      "      dtype: float32\n",
      "      name: input_9\n",
      "      sparse: false\n",
      "    inbound_nodes: []\n",
      "    name: input_9\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: linear\n",
      "      activity_regularizer: null\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: VarianceScaling\n",
      "        config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "      kernel_regularizer: null\n",
      "      name: dense_17\n",
      "      trainable: true\n",
      "      units: 512\n",
      "      use_bias: true\n",
      "    inbound_nodes:\n",
      "    - - - input_9\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: dense_17\n",
      "  - class_name: Activation\n",
      "    config: {activation: relu, name: activation_17, trainable: true}\n",
      "    inbound_nodes:\n",
      "    - - - dense_17\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: activation_17\n",
      "  - class_name: Dropout\n",
      "    config: {name: dropout_9, noise_shape: null, rate: 0.5, seed: null, trainable: true}\n",
      "    inbound_nodes:\n",
      "    - - - activation_17\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: dropout_9\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: linear\n",
      "      activity_regularizer: null\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: VarianceScaling\n",
      "        config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "      kernel_regularizer: null\n",
      "      name: dense_18\n",
      "      trainable: true\n",
      "      units: !!python/object/apply:numpy.core.multiarray.scalar\n",
      "      - !!python/object/apply:numpy.dtype\n",
      "        args: [i4, 0, 1]\n",
      "        state: !!python/tuple [3, <, null, null, null, -1, -1, 0]\n",
      "      - !!binary |\n",
      "        FAAAAA==\n",
      "      use_bias: true\n",
      "    inbound_nodes:\n",
      "    - - - dropout_9\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: dense_18\n",
      "  - class_name: Activation\n",
      "    config: {activation: softmax, name: activation_18, trainable: true}\n",
      "    inbound_nodes:\n",
      "    - - - dense_18\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: activation_18\n",
      "  name: model_9\n",
      "  output_layers:\n",
      "  - [activation_18, 0, 0]\n",
      "keras_version: 2.2.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10182 samples, validate on 1132 samples\n",
      "Epoch 1/5\n",
      "10182/10182 [==============================] - 11s 1ms/step - loss: 1.5601 - acc: 0.6196 - val_loss: 0.6467 - val_acc: 0.8604\n",
      "Epoch 2/5\n",
      "10182/10182 [==============================] - 12s 1ms/step - loss: 0.4438 - acc: 0.9019 - val_loss: 0.4446 - val_acc: 0.8922\n",
      "Epoch 3/5\n",
      "10182/10182 [==============================] - 11s 1ms/step - loss: 0.2292 - acc: 0.9539 - val_loss: 0.3786 - val_acc: 0.8940\n",
      "Epoch 4/5\n",
      "10182/10182 [==============================] - 11s 1ms/step - loss: 0.1367 - acc: 0.9777 - val_loss: 0.3543 - val_acc: 0.9019\n",
      "Epoch 5/5\n",
      "10182/10182 [==============================] - 14s 1ms/step - loss: 0.0868 - acc: 0.9890 - val_loss: 0.3398 - val_acc: 0.9028\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532/7532 [==============================] - 2s 306us/step\n",
      "\n",
      "\n",
      "Test score: 0.7093810075087809\n",
      "Test accuracy: 0.8043016463090813\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
